// Copyright 2020 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     https://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

/* eslint-disable @typescript-eslint/no-var-requires */
/* eslint-disable node/no-extraneous-import */

import {Application} from 'probot';
import {parseRegionTags} from './region-tag-parser';
import {START_TAG_REGEX} from './region-tag-parser';
import {logger} from 'gcf-utils';
import fetch from 'node-fetch';
import tmp from 'tmp-promise';
import {PullsListFilesResponseData} from '@octokit/types';
import * as minimatch from 'minimatch';
import parseDiff from 'parse-diff';

import tar from 'tar';
import util from 'util';
import fs from 'fs';
import {promises as pfs} from 'fs';
import path from 'path';

const streamPipeline = util.promisify(require('stream').pipeline);

type Conclusion =
  | 'success'
  | 'failure'
  | 'neutral'
  | 'cancelled'
  | 'timed_out'
  | 'action_required'
  | undefined;

type ChangeTypes = 'add' | 'del';

interface Change {
  type: ChangeTypes;
  region_tag: string;
  owner: string;
  repo: string;
  file?: string;
  sha: string;
  line: number;
}

interface ConfigurationOptions {
  ignoreFiles: string[];
}

const DEFAULT_CONFIGURATION: ConfigurationOptions = {
  ignoreFiles: [],
};

const CONFIGURATION_FILE_PATH = 'snippet-bot.yml';

const FULL_SCAN_ISSUE_TITLE = 'snippet-bot full scan';

class Configuration {
  private options: ConfigurationOptions;
  private minimatches: minimatch.IMinimatch[];

  constructor(options: ConfigurationOptions) {
    this.options = options;
    this.minimatches = options.ignoreFiles.map(pattern => {
      return new minimatch.Minimatch(pattern);
    });
  }

  ignoredFile(filename: string): boolean {
    return this.minimatches.some(mm => {
      return mm.match(filename);
    });
  }
}

function formatBody(
  originalBody: string,
  commentMark: string,
  addition: string
): string {
  // First cut off if we already have the commentMark.
  const markIndex = originalBody.indexOf(commentMark);
  if (markIndex >= 0) {
    originalBody = originalBody.substr(0, markIndex);
  }
  return `${originalBody}${commentMark}

${addition}

---
Report generated by [snippet-bot](https://github.com/apps/snippet-bot).
If you find problems with this result, please file an issue at:
https://github.com/googleapis/repo-automation-bots/issues.
`;
}

function formatExpandable(summary: string, detail: string): string {
  return `<details>
  <summary>${summary}</summary>

  ${detail}
</details>

`;
}

function formatChangedFile(change: Change): string {
  const url = `https://github.com/${change.owner}/${change.repo}/blob/${change.sha}/${change.file}#L${change.line}`;
  return `[\`${change.region_tag}\` in \`${change.file}\`](${url})`;
}

async function downloadFile(url: string, file: string) {
  const response = await fetch(url);
  if (response.ok) {
    return streamPipeline(response.body, fs.createWriteStream(file));
  }
  throw new Error(`unexpected response ${response.statusText}`);
}

async function getFiles(dir: string, allFiles: string[]) {
  const files = (await pfs.readdir(dir)).map(f => path.join(dir, f));
  for (const f of files) {
    if (!(await pfs.stat(f)).isDirectory()) {
      allFiles.push(f);
    }
  }
  await Promise.all(
    files.map(
      async f => (await pfs.stat(f)).isDirectory() && getFiles(f, allFiles)
    )
  );
  return allFiles;
}

export = (app: Application) => {
  app.on(
    [
      'issues.opened',
      'issues.reopened',
      'pull_request.opened',
      'pull_request.reopened',
      'pull_request.edited',
      'pull_request.synchronize',
    ],
    async context => {
      const repoUrl = context.payload.repository.full_name;
      const defaultBranch = context.payload.repository.default_branch;
      let configOptions!: ConfigurationOptions | null;
      try {
        configOptions = await context.config<ConfigurationOptions>(
          CONFIGURATION_FILE_PATH
        );
      } catch (err) {
        err.message = `Error reading configuration: ${err.message}`;
        logger.error(err);
        // Now this bot is only enabled if it finds the configuration file.
        // Exiting.
        return;
      }

      if (configOptions === null) {
        logger.info(`snippet-bot is not configured for ${repoUrl}.`);
        return;
      }
      const configuration = new Configuration({
        ...DEFAULT_CONFIGURATION,
        ...configOptions,
      });
      logger.info({config: configuration});
      const installationId = context.payload.installation.id;
      const commentMark = `<!-- probot comment [${installationId}]-->`;
      const owner = context.payload.repository.owner.login;
      const repo = context.payload.repository.name;

      if (context.payload.issue?.title.includes(FULL_SCAN_ISSUE_TITLE)) {
        // full scan start
        const issueNumber = context.payload.issue.number;

        const url = `https://github.com/${owner}/${repo}/tarball/${defaultBranch}`;
        const tmpDir = tmp.dirSync();
        logger.info(`working directory: ${tmpDir.name}`);

        const file = `${tmpDir.name}/${repo}.tar.gz`;
        // Download the default branch tarball and run full scan.
        try {
          await downloadFile(url, file);
          logger.info(`Downloaded to ${file}`);
          tar.x({
            file: file,
            cwd: tmpDir.name,
            sync: true,
          });
          let archiveDir!: string;
          for (const f of await pfs.readdir(tmpDir.name)) {
            const cur = tmpDir.name + '/' + f;
            const stat = await pfs.lstat(cur);
            if (stat.isDirectory()) {
              archiveDir = cur;
            }
          }
          if (archiveDir === undefined) {
            throw new Error('Failed to extract the archive');
          }
          // Determine the short commit hash from the directory name.
          // We'll use the hash for creating permalink.
          let commitHash = defaultBranch; // Defaulting to the default branch.
          const lastDashIndex = archiveDir.lastIndexOf('-');
          if (lastDashIndex !== -1) {
            commitHash = archiveDir.substr(lastDashIndex + 1);
          }
          logger.info(`Using commit hash "${commitHash}"`);
          const files = await getFiles(archiveDir, []);

          let mismatchedTags = false;
          const failureMessages: string[] = [];

          for (const file of files) {
            if (configuration.ignoredFile(file)) {
              logger.info('ignoring file from configuration: ' + file);
              continue;
            }
            try {
              const fileContents = await pfs.readFile(file, 'utf-8');
              const parseResult = parseRegionTags(
                fileContents,
                file.replace(archiveDir + '/', '')
              );
              if (!parseResult.result) {
                mismatchedTags = true;
                for (const message of parseResult.messages) {
                  // Create a link to the source code at the commit.
                  const linkedMessage = message.replace(
                    /^(.+):(\d+),/,
                    `- [ ] [$1:$2](https://github.com/${owner}/${repo}/blob/${commitHash}/$1#L$2),`
                  );
                  failureMessages.push(linkedMessage);
                }
                parseResult.messages.join('\n');
              }
            } catch (err) {
              err.message = `Failed to read the file: ${err.message}`;
              logger.error(err);
              continue;
            }
          }
          let bodyDetail = 'Great job! No unmatching region tags found!';
          if (mismatchedTags) {
            bodyDetail = failureMessages.join('\n');
          }
          await context.github.issues.update({
            owner: owner,
            repo: repo,
            issue_number: issueNumber,
            body: formatBody(
              context.payload.issue.body,
              commentMark,
              `## snippet-bot scan result
Life is too short to manually check unmatched region tags.
Here is the result:
${bodyDetail}`
            ),
          });
        } catch (err) {
          err.message = `Failed to scan files: ${err.message}`;
          logger.error(err);
          await context.github.issues.update({
            owner: owner,
            repo: repo,
            issue_number: issueNumber,
            body: formatBody(
              context.payload.issue.body,
              commentMark,
              `## snippet-bot scan result\nFailed running the full scan: ${err}.`
            ),
          });
        } finally {
          // Clean up the directory.
          await pfs.rmdir(tmpDir.name, {recursive: true});
        }
      } // full scan end.

      if (context.payload.pull_request === undefined) {
        return;
      }
      const sha = context.payload.pull_request.base.sha;
      const headOnwer = context.payload.pull_request.head.repo.owner.login;
      const headRepo = context.payload.pull_request.head.repo.name;
      const headSha = context.payload.pull_request.head.sha;
      // Check on pull requests.
      // List pull request files for the given PR
      // https://developer.github.com/v3/pulls/#list-pull-requests-files
      const listFilesParams = context.repo({
        pull_number: context.payload.pull_request.number,
        per_page: 100,
      });
      const pullRequestCommitSha = context.payload.pull_request.head.sha;
      logger.info({sha: pullRequestCommitSha});
      // TODO: handle pagination
      let files: PullsListFilesResponseData;
      try {
        files = (await context.github.pulls.listFiles(listFilesParams)).data;
      } catch (err) {
        logger.error('---------------------');
        logger.error(err);
        return;
      }

      let mismatchedTags = false;
      let tagsFound = false;
      const failureMessages: string[] = [];

      // If we found any new files, verify they all have matching region tags.
      for (let i = 0; files[i] !== undefined; i++) {
        const file = files[i];

        if (configuration.ignoredFile(file.filename)) {
          logger.info('ignoring file from configuration: ' + file.filename);
          continue;
        }

        if (file.status === 'removed') {
          logger.info('ignoring deleted file: ' + file.filename);
          continue;
        }

        const blob = await context.github.git.getBlob(
          context.repo({
            file_sha: file.sha,
          })
        );

        const fileContents = Buffer.from(blob.data.content, 'base64').toString(
          'utf8'
        );

        const parseResult = parseRegionTags(fileContents, file.filename);
        if (!parseResult.result) {
          mismatchedTags = true;
          failureMessages.push(parseResult.messages.join('\n'));
        }
        if (parseResult.tagsFound) {
          tagsFound = true;
        }
      }

      const checkParams = context.repo({
        name: 'Mismatched region tag',
        conclusion: 'success' as Conclusion,
        head_sha: pullRequestCommitSha,
        output: {
          title: 'Region tag check',
          summary: 'Region tag successful',
          text: 'Region tag successful',
        },
      });

      if (mismatchedTags) {
        checkParams.conclusion = 'failure';
        checkParams.output = {
          title: 'Mismatched region tag detected.',
          summary: 'Some new files have mismatched region tag',
          text: failureMessages.join('\n'),
        };
      }

      // post the status of commit linting to the PR, using:
      // https://developer.github.com/v3/checks/
      if (tagsFound) {
        await context.github.checks.create(checkParams);
      }

      // Parse the PR diff and recognize added/deleted region tags.
      const response = await fetch(context.payload.pull_request.diff_url);
      const diff = await response.text();
      const diffResult = parseDiff(diff);
      const changes: Change[] = [];
      let added = 0;
      let deleted = 0;
      for (const file of diffResult) {
        for (const chunk of file.chunks) {
          for (const change of chunk.changes) {
            if (change.type === 'normal') {
              continue;
            }
            // We only track add/deletion of start tags.
            const startMatch = change.content.match(START_TAG_REGEX);
            // TODO: change owner and repo for deletion
            if (startMatch) {
              if (change.type === 'add') {
                added += 1;
              }
              if (change.type === 'del') {
                deleted += 1;
              }
              changes.push({
                type: change.type === 'del' ? 'del' : 'add',
                region_tag: startMatch[1],
                owner: change.type === 'del' ? owner : headOnwer,
                repo: change.type === 'del' ? repo : headRepo,
                file: change.type === 'del' ? file.from : file.to,
                sha: change.type === 'del' ? sha : headSha,
                line: change.ln,
              });
            }
          }
        }
      }
      if (changes.length === 0) {
        return;
      }

      // Add or update a comment on the PR.
      const prNumber = context.payload.pull_request.number;
      let commentBody = 'Here is the summary of changes.\n';
      if (added > 0) {
        const plural = added === 1 ? '' : 's';
        const summary = `You added ${added} region tag${plural}.`;
        let detail = '';
        for (const change of changes) {
          if (change.type === 'add') {
            detail += `- ${formatChangedFile(change)}\n`;
          }
        }
        commentBody += formatExpandable(summary, detail);
      }
      if (deleted > 0) {
        const plural = deleted === 1 ? '' : 's';
        const summary = `You deleted ${deleted} region tag${plural}.\n`;
        let detail = '';
        for (const change of changes) {
          if (change.type === 'del') {
            detail += `- ${formatChangedFile(change)}\n`;
          }
        }
        commentBody += formatExpandable(summary, detail);
      }

      const listCommentsResponse = await context.github.issues.listComments({
        owner: owner,
        repo: repo,
        per_page: 50,
        issue_number: prNumber,
      });
      let found = false;
      for (const comment of listCommentsResponse.data) {
        if (comment.body.includes(commentMark)) {
          // We found the existing comment, so updating it
          await context.github.issues.updateComment({
            owner: owner,
            repo: repo,
            comment_id: comment.id,
            body: `${commentMark}\n${commentBody}`,
          });
          found = true;
        }
      }
      if (!found) {
        await context.github.issues.createComment({
          owner: owner,
          repo: repo,
          issue_number: prNumber,
          body: `${commentMark}\n${commentBody}`,
        });
      }
    }
  );
};
